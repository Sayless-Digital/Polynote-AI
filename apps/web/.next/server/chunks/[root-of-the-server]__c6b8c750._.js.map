{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 3, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 59, "column": 0}, "map": {"version":3,"sources":["file:///home/slk/Documents/Projects/polynote/packages/db/src/schema.ts"],"sourcesContent":["import { pgTable, text, timestamp, uuid, jsonb, index } from 'drizzle-orm/pg-core';\nimport { relations } from 'drizzle-orm';\n\n// Notes table\nexport const notes = pgTable('notes', {\n  id: uuid('id').primaryKey().defaultRandom(),\n  title: text('title').notNull(),\n  content: text('content').notNull(),\n  transcript: text('transcript'), // Original voice transcript\n  summary: text('summary'), // AI-generated summary\n  tags: jsonb('tags').$type<string[]>(), // AI-generated tags\n  categories: jsonb('categories').$type<string[]>(), // AI-generated categories\n  metadata: jsonb('metadata').$type<Record<string, unknown>>(), // Additional AI metadata\n  createdAt: timestamp('created_at').defaultNow().notNull(),\n  updatedAt: timestamp('updated_at').defaultNow().notNull(),\n}, (table) => ({\n  titleIdx: index('notes_title_idx').on(table.title),\n  createdAtIdx: index('notes_created_at_idx').on(table.createdAt),\n  tagsIdx: index('notes_tags_idx').on(table.tags),\n  categoriesIdx: index('notes_categories_idx').on(table.categories),\n}));\n\n// Note searches table for full-text search\nexport const noteSearches = pgTable('note_searches', {\n  id: uuid('id').primaryKey().defaultRandom(),\n  noteId: uuid('note_id').references(() => notes.id, { onDelete: 'cascade' }),\n  searchVector: text('search_vector'), // For full-text search\n  createdAt: timestamp('created_at').defaultNow().notNull(),\n});\n\n// File attachments table\nexport const attachments = pgTable('attachments', {\n  id: uuid('id').primaryKey().defaultRandom(),\n  noteId: uuid('note_id').references(() => notes.id, { onDelete: 'cascade' }).notNull(),\n  filename: text('filename').notNull(),\n  originalName: text('original_name').notNull(),\n  size: text('size').notNull(), // File size in bytes\n  type: text('type').notNull(), // MIME type\n  url: text('url').notNull(), // Vercel Blob URL\n  content: text('content'), // Extracted text content for search\n  createdAt: timestamp('created_at').defaultNow().notNull(),\n}, (table) => ({\n  noteIdIdx: index('attachments_note_id_idx').on(table.noteId),\n  createdAtIdx: index('attachments_created_at_idx').on(table.createdAt),\n}));\n\n// Relations\nexport const notesRelations = relations(notes, ({ many }) => ({\n  searches: many(noteSearches),\n  attachments: many(attachments),\n}));\n\nexport const noteSearchesRelations = relations(noteSearches, ({ one }) => ({\n  note: one(notes, {\n    fields: [noteSearches.noteId],\n    references: [notes.id],\n  }),\n}));\n\nexport const attachmentsRelations = relations(attachments, ({ one }) => ({\n  note: one(notes, {\n    fields: [attachments.noteId],\n    references: [notes.id],\n  }),\n}));\n\n// Types\nexport type Note = typeof notes.$inferSelect;\nexport type NewNote = typeof notes.$inferInsert;\nexport type NoteSearch = typeof noteSearches.$inferSelect;\nexport type NewNoteSearch = typeof noteSearches.$inferInsert;\nexport type Attachment = typeof attachments.$inferSelect;\nexport type NewAttachment = typeof attachments.$inferInsert;\n"],"names":[],"mappings":";;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AACA;;;AAGO,MAAM,QAAQ,IAAA,kKAAO,EAAC,SAAS;IACpC,IAAI,IAAA,yKAAI,EAAC,MAAM,UAAU,GAAG,aAAa;IACzC,OAAO,IAAA,yKAAI,EAAC,SAAS,OAAO;IAC5B,SAAS,IAAA,yKAAI,EAAC,WAAW,OAAO;IAChC,YAAY,IAAA,yKAAI,EAAC;IACjB,SAAS,IAAA,yKAAI,EAAC;IACd,MAAM,IAAA,2KAAK,EAAC,QAAQ,KAAK;IACzB,YAAY,IAAA,2KAAK,EAAC,cAAc,KAAK;IACrC,UAAU,IAAA,2KAAK,EAAC,YAAY,KAAK;IACjC,WAAW,IAAA,mLAAS,EAAC,cAAc,UAAU,GAAG,OAAO;IACvD,WAAW,IAAA,mLAAS,EAAC,cAAc,UAAU,GAAG,OAAO;AACzD,GAAG,CAAC,QAAU,CAAC;QACb,UAAU,IAAA,kKAAK,EAAC,mBAAmB,EAAE,CAAC,MAAM,KAAK;QACjD,cAAc,IAAA,kKAAK,EAAC,wBAAwB,EAAE,CAAC,MAAM,SAAS;QAC9D,SAAS,IAAA,kKAAK,EAAC,kBAAkB,EAAE,CAAC,MAAM,IAAI;QAC9C,eAAe,IAAA,kKAAK,EAAC,wBAAwB,EAAE,CAAC,MAAM,UAAU;IAClE,CAAC;AAGM,MAAM,eAAe,IAAA,kKAAO,EAAC,iBAAiB;IACnD,IAAI,IAAA,yKAAI,EAAC,MAAM,UAAU,GAAG,aAAa;IACzC,QAAQ,IAAA,yKAAI,EAAC,WAAW,UAAU,CAAC,IAAM,MAAM,EAAE,EAAE;QAAE,UAAU;IAAU;IACzE,cAAc,IAAA,yKAAI,EAAC;IACnB,WAAW,IAAA,mLAAS,EAAC,cAAc,UAAU,GAAG,OAAO;AACzD;AAGO,MAAM,cAAc,IAAA,kKAAO,EAAC,eAAe;IAChD,IAAI,IAAA,yKAAI,EAAC,MAAM,UAAU,GAAG,aAAa;IACzC,QAAQ,IAAA,yKAAI,EAAC,WAAW,UAAU,CAAC,IAAM,MAAM,EAAE,EAAE;QAAE,UAAU;IAAU,GAAG,OAAO;IACnF,UAAU,IAAA,yKAAI,EAAC,YAAY,OAAO;IAClC,cAAc,IAAA,yKAAI,EAAC,iBAAiB,OAAO;IAC3C,MAAM,IAAA,yKAAI,EAAC,QAAQ,OAAO;IAC1B,MAAM,IAAA,yKAAI,EAAC,QAAQ,OAAO;IAC1B,KAAK,IAAA,yKAAI,EAAC,OAAO,OAAO;IACxB,SAAS,IAAA,yKAAI,EAAC;IACd,WAAW,IAAA,mLAAS,EAAC,cAAc,UAAU,GAAG,OAAO;AACzD,GAAG,CAAC,QAAU,CAAC;QACb,WAAW,IAAA,kKAAK,EAAC,2BAA2B,EAAE,CAAC,MAAM,MAAM;QAC3D,cAAc,IAAA,kKAAK,EAAC,8BAA8B,EAAE,CAAC,MAAM,SAAS;IACtE,CAAC;AAGM,MAAM,iBAAiB,IAAA,0JAAS,EAAC,OAAO,CAAC,EAAE,IAAI,EAAE,GAAK,CAAC;QAC5D,UAAU,KAAK;QACf,aAAa,KAAK;IACpB,CAAC;AAEM,MAAM,wBAAwB,IAAA,0JAAS,EAAC,cAAc,CAAC,EAAE,GAAG,EAAE,GAAK,CAAC;QACzE,MAAM,IAAI,OAAO;YACf,QAAQ;gBAAC,aAAa,MAAM;aAAC;YAC7B,YAAY;gBAAC,MAAM,EAAE;aAAC;QACxB;IACF,CAAC;AAEM,MAAM,uBAAuB,IAAA,0JAAS,EAAC,aAAa,CAAC,EAAE,GAAG,EAAE,GAAK,CAAC;QACvE,MAAM,IAAI,OAAO;YACf,QAAQ;gBAAC,YAAY,MAAM;aAAC;YAC5B,YAAY;gBAAC,MAAM,EAAE;aAAC;QACxB;IACF,CAAC","debugId":null}},
    {"offset": {"line": 153, "column": 0}, "map": {"version":3,"sources":["file:///home/slk/Documents/Projects/polynote/packages/db/src/index.ts"],"sourcesContent":["import { neon } from '@neondatabase/serverless';\nimport { drizzle } from 'drizzle-orm/neon-http';\nimport * as schema from './schema';\n\nconst sql = neon(process.env.DATABASE_URL!);\nexport const db = drizzle(sql, { schema });\n\nexport * from './schema';\n"],"names":[],"mappings":";;;;AAAA;AACA;AACA;;;;;;;;;AAEA,MAAM,MAAM,IAAA,gKAAI,EAAC,QAAQ,GAAG,CAAC,YAAY;AAClC,MAAM,KAAK,IAAA,qKAAO,EAAC,KAAK;IAAE,QAAA;AAAO","debugId":null}},
    {"offset": {"line": 178, "column": 0}, "map": {"version":3,"sources":["file:///home/slk/Documents/Projects/polynote/apps/web/src/lib/ai.ts"],"sourcesContent":["import { google } from '@ai-sdk/google';\nimport { generateText, generateObject } from 'ai';\nimport { z } from 'zod';\n\n// Initialize Gemini Flash model with explicit API key\nexport const geminiFlash = google('models/gemini-1.5-flash-latest', {\n  apiKey: process.env.GOOGLE_GENERATIVE_AI_API_KEY,\n});\n\n// Schema for note analysis\nconst NoteAnalysisSchema = z.object({\n  title: z.string(),\n  summary: z.string(),\n  tags: z.array(z.string()),\n  categories: z.array(z.string()),\n  sentiment: z.string(),\n  keyPoints: z.array(z.string()),\n});\n\n// Schema for search queries\nconst SearchQuerySchema = z.object({\n  intent: z.string(),\n  keywords: z.array(z.string()),\n  filters: z.object({\n    categories: z.array(z.string()).optional(),\n    tags: z.array(z.string()).optional(),\n    dateRange: z.string().optional(),\n  }),\n});\n\n/**\n * Analyze a note transcript and extract metadata\n */\nexport async function analyzeNote(transcript: string) {\n  const prompt = `\nAnalyze this note content and provide structured information. The content may include:\n1. User's typed note text\n2. File attachments with content in the format \"--- Content from [filename] ---\"\n\nContent: \"${transcript}\"\n\nPlease analyze ALL content (both user text and file attachments) and provide:\n- A concise title that captures the main theme (consider both user text and file content)\n- A brief summary that synthesizes the user's intent with the file content\n- Relevant tags (3-5 keywords) from both user text and file content\n- Categories this note belongs to based on the complete content\n- Overall sentiment\n- Key points mentioned across all content\n\nFocus on the user's intent and how the file content relates to it. If there are file attachments, consider them as part of the note's context and meaning.\n`;\n\n  try {\n    const result = await generateObject({\n      model: geminiFlash,\n      schema: NoteAnalysisSchema,\n      prompt,\n    });\n\n    return result.object;\n  } catch (error) {\n    console.error('Error analyzing note:', error);\n    // Fallback analysis\n    return {\n      title: transcript.split(' ').slice(0, 5).join(' ') + '...',\n      summary: transcript.length > 100 ? transcript.substring(0, 100) + '...' : transcript,\n      tags: ['note'],\n      categories: ['general'],\n      sentiment: 'neutral',\n      keyPoints: [transcript],\n    };\n  }\n}\n\n/**\n * Generate search suggestions based on user query\n */\nexport async function generateSearchQuery(query: string) {\n  const prompt = `\nAnalyze this search query and provide structured search parameters:\n\nQuery: \"${query}\"\n\nPlease identify:\n- The user's intent\n- Key search keywords\n- Any category or tag filters mentioned\n- Date range preferences if any\n\nMake the search parameters specific and helpful.\n`;\n\n  try {\n    const result = await generateObject({\n      model: geminiFlash,\n      schema: SearchQuerySchema,\n      prompt,\n    });\n\n    return result.object;\n  } catch (error) {\n    console.error('Error generating search query:', error);\n    return {\n      intent: 'general_search',\n      keywords: query.split(' '),\n      filters: {},\n    };\n  }\n}\n\n/**\n * Generate a response to user questions about their notes\n */\nexport async function generateNoteResponse(question: string, context: string) {\n  const prompt = `\nBased on the following note content, answer the user's question:\n\nNote Content: \"${context}\"\n\nUser Question: \"${question}\"\n\nProvide a helpful, concise answer based on the note content.\n`;\n\n  try {\n    const result = await generateText({\n      model: geminiFlash,\n      prompt,\n    });\n\n    return result.text;\n  } catch (error) {\n    console.error('Error generating response:', error);\n    return 'I apologize, but I encountered an error processing your question.';\n  }\n}\n\n/**\n * Test AI connection and API key\n */\nexport async function testAIConnection(): Promise<boolean> {\n  try {\n    const result = await generateText({\n      model: geminiFlash,\n      prompt: 'Say \"Hello\" if you can read this message.',\n    });\n\n    return result.text.includes('Hello') || result.text.length > 0;\n  } catch (error) {\n    console.error('AI connection test failed:', error);\n    return false;\n  }\n}\n\n/**\n * Transcribe audio to text (placeholder for voice transcription)\n */\nexport async function transcribeAudio(): Promise<string> {\n  // This is a placeholder - in a real implementation, you'd use:\n  // - Web Speech API for browser-based transcription\n  // - Google Speech-to-Text API\n  // - OpenAI Whisper API\n  // For now, return a mock transcript\n  return new Promise((resolve) => {\n    setTimeout(() => {\n      resolve(\"This is a mock transcript from the audio. In a real implementation, this would be the actual transcribed text from the user's voice recording.\");\n    }, 1000);\n  });\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;AAAA;AACA;AACA;;;;AAGO,MAAM,cAAc,IAAA,mKAAM,EAAC,kCAAkC;IAClE,QAAQ,QAAQ,GAAG,CAAC,4BAA4B;AAClD;AAEA,2BAA2B;AAC3B,MAAM,qBAAqB,oLAAC,CAAC,MAAM,CAAC;IAClC,OAAO,oLAAC,CAAC,MAAM;IACf,SAAS,oLAAC,CAAC,MAAM;IACjB,MAAM,oLAAC,CAAC,KAAK,CAAC,oLAAC,CAAC,MAAM;IACtB,YAAY,oLAAC,CAAC,KAAK,CAAC,oLAAC,CAAC,MAAM;IAC5B,WAAW,oLAAC,CAAC,MAAM;IACnB,WAAW,oLAAC,CAAC,KAAK,CAAC,oLAAC,CAAC,MAAM;AAC7B;AAEA,4BAA4B;AAC5B,MAAM,oBAAoB,oLAAC,CAAC,MAAM,CAAC;IACjC,QAAQ,oLAAC,CAAC,MAAM;IAChB,UAAU,oLAAC,CAAC,KAAK,CAAC,oLAAC,CAAC,MAAM;IAC1B,SAAS,oLAAC,CAAC,MAAM,CAAC;QAChB,YAAY,oLAAC,CAAC,KAAK,CAAC,oLAAC,CAAC,MAAM,IAAI,QAAQ;QACxC,MAAM,oLAAC,CAAC,KAAK,CAAC,oLAAC,CAAC,MAAM,IAAI,QAAQ;QAClC,WAAW,oLAAC,CAAC,MAAM,GAAG,QAAQ;IAChC;AACF;AAKO,eAAe,YAAY,UAAkB;IAClD,MAAM,SAAS,CAAC;;;;;UAKR,EAAE,WAAW;;;;;;;;;;;AAWvB,CAAC;IAEC,IAAI;QACF,MAAM,SAAS,MAAM,IAAA,wKAAc,EAAC;YAClC,OAAO;YACP,QAAQ;YACR;QACF;QAEA,OAAO,OAAO,MAAM;IACtB,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,yBAAyB;QACvC,oBAAoB;QACpB,OAAO;YACL,OAAO,WAAW,KAAK,CAAC,KAAK,KAAK,CAAC,GAAG,GAAG,IAAI,CAAC,OAAO;YACrD,SAAS,WAAW,MAAM,GAAG,MAAM,WAAW,SAAS,CAAC,GAAG,OAAO,QAAQ;YAC1E,MAAM;gBAAC;aAAO;YACd,YAAY;gBAAC;aAAU;YACvB,WAAW;YACX,WAAW;gBAAC;aAAW;QACzB;IACF;AACF;AAKO,eAAe,oBAAoB,KAAa;IACrD,MAAM,SAAS,CAAC;;;QAGV,EAAE,MAAM;;;;;;;;;AAShB,CAAC;IAEC,IAAI;QACF,MAAM,SAAS,MAAM,IAAA,wKAAc,EAAC;YAClC,OAAO;YACP,QAAQ;YACR;QACF;QAEA,OAAO,OAAO,MAAM;IACtB,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,kCAAkC;QAChD,OAAO;YACL,QAAQ;YACR,UAAU,MAAM,KAAK,CAAC;YACtB,SAAS,CAAC;QACZ;IACF;AACF;AAKO,eAAe,qBAAqB,QAAgB,EAAE,OAAe;IAC1E,MAAM,SAAS,CAAC;;;eAGH,EAAE,QAAQ;;gBAET,EAAE,SAAS;;;AAG3B,CAAC;IAEC,IAAI;QACF,MAAM,SAAS,MAAM,IAAA,sKAAY,EAAC;YAChC,OAAO;YACP;QACF;QAEA,OAAO,OAAO,IAAI;IACpB,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,8BAA8B;QAC5C,OAAO;IACT;AACF;AAKO,eAAe;IACpB,IAAI;QACF,MAAM,SAAS,MAAM,IAAA,sKAAY,EAAC;YAChC,OAAO;YACP,QAAQ;QACV;QAEA,OAAO,OAAO,IAAI,CAAC,QAAQ,CAAC,YAAY,OAAO,IAAI,CAAC,MAAM,GAAG;IAC/D,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,8BAA8B;QAC5C,OAAO;IACT;AACF;AAKO,eAAe;IACpB,+DAA+D;IAC/D,mDAAmD;IACnD,8BAA8B;IAC9B,uBAAuB;IACvB,oCAAoC;IACpC,OAAO,IAAI,QAAQ,CAAC;QAClB,WAAW;YACT,QAAQ;QACV,GAAG;IACL;AACF","debugId":null}},
    {"offset": {"line": 345, "column": 0}, "map": {"version":3,"sources":["file:///home/slk/Documents/Projects/polynote/apps/web/src/app/api/notes/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from 'next/server';\nimport { db } from '@polynote/db';\nimport { notes, attachments } from '@polynote/db';\nimport { analyzeNote } from '@/lib/ai';\nimport { sql, desc, and, or, ilike, eq } from 'drizzle-orm';\n\nexport async function GET(request: NextRequest) {\n  try {\n    const { searchParams } = new URL(request.url);\n    const search = searchParams.get('search');\n    const category = searchParams.get('category');\n    const tag = searchParams.get('tag');\n\n    // Build conditions array for better query optimization\n    const conditions = [];\n\n    // Optimize search with proper indexing\n    if (search) {\n      const searchTerm = `%${search}%`;\n      conditions.push(\n        or(\n          ilike(notes.title, searchTerm),\n          ilike(notes.content, searchTerm)\n        )\n      );\n    }\n\n    // Optimize category filter\n    if (category) {\n      conditions.push(sql`${notes.categories}::text LIKE ${`%${category}%`}`);\n    }\n\n    // Optimize tag filter\n    if (tag) {\n      conditions.push(sql`${notes.tags}::text LIKE ${`%${tag}%`}`);\n    }\n\n    // Build optimized query\n    let query = db.select().from(notes);\n    \n    if (conditions.length > 0) {\n      query = query.where(and(...conditions));\n    }\n\n    const result = await query\n      .orderBy(desc(notes.createdAt))\n      .limit(50); // Reduced limit for better performance\n\n    // Add optimized cache headers and compression\n    const response = NextResponse.json(result);\n    response.headers.set('Cache-Control', 'public, max-age=60, stale-while-revalidate=120');\n    response.headers.set('ETag', `\"notes-${Date.now()}\"`);\n    response.headers.set('Vary', 'Accept-Encoding');\n    \n    return response;\n  } catch (error) {\n    console.error('Error fetching notes:', error);\n    return NextResponse.json(\n      { error: 'Failed to fetch notes' },\n      { status: 500 }\n    );\n  }\n}\n\nexport async function POST(request: NextRequest) {\n  try {\n    const body = await request.json();\n    const { title, content, transcript, tags, categories, attachments: uploadedFiles, aiAnalysis } = body;\n\n    if (!content?.trim()) {\n      return NextResponse.json(\n        { error: 'Content is required' },\n        { status: 400 }\n      );\n    }\n\n    // Use AI analysis from frontend if provided, otherwise do our own analysis\n    let finalAiAnalysis = aiAnalysis;\n    \n    if (!finalAiAnalysis && content.trim()) {\n      try {\n        const contentToAnalyze = transcript || content;\n        finalAiAnalysis = await analyzeNote(contentToAnalyze);\n      } catch (aiError) {\n        console.error('AI analysis failed:', aiError);\n      }\n    }\n\n    // Prepare the note data - prioritize AI-generated title and summary\n    const noteData = {\n      title: finalAiAnalysis?.title || title || 'Untitled Note',\n      content,\n      transcript: transcript || null,\n      summary: finalAiAnalysis?.summary || null,\n      tags: finalAiAnalysis?.tags || tags || [],\n      categories: finalAiAnalysis?.categories || categories || [],\n      metadata: finalAiAnalysis ? {\n        sentiment: finalAiAnalysis.sentiment,\n        keyPoints: finalAiAnalysis.keyPoints,\n      } : null,\n    };\n\n    // Save to database\n    const result = await db.insert(notes).values(noteData).returning();\n    const savedNote = result[0];\n\n    // Save file attachments if any\n    if (uploadedFiles && uploadedFiles.length > 0) {\n      const attachmentData = uploadedFiles.map((file: any) => ({\n        noteId: savedNote.id,\n        filename: file.filename,\n        originalName: file.originalName,\n        size: file.size.toString(),\n        type: file.type,\n        url: file.url,\n        content: file.content || null, // Store extracted content for search\n      }));\n\n      await db.insert(attachments).values(attachmentData);\n    }\n\n    return NextResponse.json(savedNote);\n  } catch (error) {\n    console.error('Error creating note:', error);\n    return NextResponse.json(\n      { error: 'Failed to create note' },\n      { status: 500 }\n    );\n  }\n}\n"],"names":[],"mappings":";;;;;;AAAA;AACA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;;;AAEO,eAAe,IAAI,OAAoB;IAC5C,IAAI;QACF,MAAM,EAAE,YAAY,EAAE,GAAG,IAAI,IAAI,QAAQ,GAAG;QAC5C,MAAM,SAAS,aAAa,GAAG,CAAC;QAChC,MAAM,WAAW,aAAa,GAAG,CAAC;QAClC,MAAM,MAAM,aAAa,GAAG,CAAC;QAE7B,uDAAuD;QACvD,MAAM,aAAa,EAAE;QAErB,uCAAuC;QACvC,IAAI,QAAQ;YACV,MAAM,aAAa,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC;YAChC,WAAW,IAAI,CACb,IAAA,0KAAE,EACA,IAAA,6KAAK,EAAC,0IAAK,CAAC,KAAK,EAAE,aACnB,IAAA,6KAAK,EAAC,0IAAK,CAAC,OAAO,EAAE;QAG3B;QAEA,2BAA2B;QAC3B,IAAI,UAAU;YACZ,WAAW,IAAI,CAAC,qJAAG,CAAC,EAAE,0IAAK,CAAC,UAAU,CAAC,YAAY,EAAE,CAAC,CAAC,EAAE,SAAS,CAAC,CAAC,CAAC,CAAC;QACxE;QAEA,sBAAsB;QACtB,IAAI,KAAK;YACP,WAAW,IAAI,CAAC,qJAAG,CAAC,EAAE,0IAAK,CAAC,IAAI,CAAC,YAAY,EAAE,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC;QAC7D;QAEA,wBAAwB;QACxB,IAAI,QAAQ,sJAAE,CAAC,MAAM,GAAG,IAAI,CAAC,0IAAK;QAElC,IAAI,WAAW,MAAM,GAAG,GAAG;YACzB,QAAQ,MAAM,KAAK,CAAC,IAAA,2KAAG,KAAI;QAC7B;QAEA,MAAM,SAAS,MAAM,MAClB,OAAO,CAAC,IAAA,wKAAI,EAAC,0IAAK,CAAC,SAAS,GAC5B,KAAK,CAAC,KAAK,uCAAuC;QAErD,8CAA8C;QAC9C,MAAM,WAAW,gJAAY,CAAC,IAAI,CAAC;QACnC,SAAS,OAAO,CAAC,GAAG,CAAC,iBAAiB;QACtC,SAAS,OAAO,CAAC,GAAG,CAAC,QAAQ,CAAC,OAAO,EAAE,KAAK,GAAG,GAAG,CAAC,CAAC;QACpD,SAAS,OAAO,CAAC,GAAG,CAAC,QAAQ;QAE7B,OAAO;IACT,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,yBAAyB;QACvC,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO;QAAwB,GACjC;YAAE,QAAQ;QAAI;IAElB;AACF;AAEO,eAAe,KAAK,OAAoB;IAC7C,IAAI;QACF,MAAM,OAAO,MAAM,QAAQ,IAAI;QAC/B,MAAM,EAAE,KAAK,EAAE,OAAO,EAAE,UAAU,EAAE,IAAI,EAAE,UAAU,EAAE,aAAa,aAAa,EAAE,UAAU,EAAE,GAAG;QAEjG,IAAI,CAAC,SAAS,QAAQ;YACpB,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAAsB,GAC/B;gBAAE,QAAQ;YAAI;QAElB;QAEA,2EAA2E;QAC3E,IAAI,kBAAkB;QAEtB,IAAI,CAAC,mBAAmB,QAAQ,IAAI,IAAI;YACtC,IAAI;gBACF,MAAM,mBAAmB,cAAc;gBACvC,kBAAkB,MAAM,IAAA,gJAAW,EAAC;YACtC,EAAE,OAAO,SAAS;gBAChB,QAAQ,KAAK,CAAC,uBAAuB;YACvC;QACF;QAEA,oEAAoE;QACpE,MAAM,WAAW;YACf,OAAO,iBAAiB,SAAS,SAAS;YAC1C;YACA,YAAY,cAAc;YAC1B,SAAS,iBAAiB,WAAW;YACrC,MAAM,iBAAiB,QAAQ,QAAQ,EAAE;YACzC,YAAY,iBAAiB,cAAc,cAAc,EAAE;YAC3D,UAAU,kBAAkB;gBAC1B,WAAW,gBAAgB,SAAS;gBACpC,WAAW,gBAAgB,SAAS;YACtC,IAAI;QACN;QAEA,mBAAmB;QACnB,MAAM,SAAS,MAAM,sJAAE,CAAC,MAAM,CAAC,0IAAK,EAAE,MAAM,CAAC,UAAU,SAAS;QAChE,MAAM,YAAY,MAAM,CAAC,EAAE;QAE3B,+BAA+B;QAC/B,IAAI,iBAAiB,cAAc,MAAM,GAAG,GAAG;YAC7C,MAAM,iBAAiB,cAAc,GAAG,CAAC,CAAC,OAAc,CAAC;oBACvD,QAAQ,UAAU,EAAE;oBACpB,UAAU,KAAK,QAAQ;oBACvB,cAAc,KAAK,YAAY;oBAC/B,MAAM,KAAK,IAAI,CAAC,QAAQ;oBACxB,MAAM,KAAK,IAAI;oBACf,KAAK,KAAK,GAAG;oBACb,SAAS,KAAK,OAAO,IAAI;gBAC3B,CAAC;YAED,MAAM,sJAAE,CAAC,MAAM,CAAC,gJAAW,EAAE,MAAM,CAAC;QACtC;QAEA,OAAO,gJAAY,CAAC,IAAI,CAAC;IAC3B,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,wBAAwB;QACtC,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO;QAAwB,GACjC;YAAE,QAAQ;QAAI;IAElB;AACF","debugId":null}}]
}